{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f989d9b-603c-4a72-859c-25e5eb94bf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from groq import Groq\n",
    "\n",
    "# ─── CLEANUP ─────────────────────────────────────────────────────\n",
    "for var in ['base_model', 'peft_model', 'groq_client']:\n",
    "    if var in globals():\n",
    "        del globals()[var]\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# ─── CONFIG ──────────────────────────────────────────────────────\n",
    "BASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "ADAPTER_PATH = \"./mistral7b_reasoning_clf_optimized/checkpoint-200\"\n",
    "OFFLOAD_DIR = \"./offload_cache\"\n",
    "CLASSIFIER_ADAPTER_NAME = \"classifier_adapter\"\n",
    "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n",
    "\n",
    "# ─── LOGGING ─────────────────────────────────────────────────────\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "if not torch.cuda.is_available():\n",
    "    raise SystemError(\"CUDA device not available.\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "# ─── TOKENIZER ───────────────────────────────────────────────────\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ─── MODEL LOAD ──────────────────────────────────────────────────\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "os.makedirs(OFFLOAD_DIR, exist_ok=True)\n",
    "\n",
    "logging.info(\"⏳ Loading base model with 4-bit quantization...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map=\"auto\",\n",
    "    max_memory={0: \"7GB\", \"cpu\": \"100GB\"},\n",
    "    offload_folder=OFFLOAD_DIR,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "logging.info(\"⏳ Attaching LoRA adapter for classification...\")\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    ADAPTER_PATH,\n",
    "    adapter_name=CLASSIFIER_ADAPTER_NAME\n",
    ")\n",
    "\n",
    "base_model.eval()\n",
    "peft_model.eval()\n",
    "logging.info(\"✅ Models ready.\")\n",
    "\n",
    "# ─── GROQ INIT ───────────────────────────────────────────────────\n",
    "if not GROQ_API_KEY:\n",
    "    raise SystemError(\"GROQ_API_KEY environment variable not set.\")\n",
    "groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "# ─── CLEAN TEXT ──────────────────────────────────────────────────\n",
    "def clean_artifacts(text: str) -> str:\n",
    "    return re.sub(r'(ACHE)+', '', text, flags=re.IGNORECASE).strip()\n",
    "\n",
    "# ─── REASONING CONFIG ────────────────────────────────────────────\n",
    "REASONING_CONFIG = {\n",
    "    \"Simple\": {\n",
    "        \"instruction\": (\n",
    "            \"Respond with a direct, concise answer. You may use a short list or a few lines \"\n",
    "            \"if needed . Do not explain or justify the answer.\"\n",
    "        ),\n",
    "        \"max_new_tokens\": 128,\n",
    "        \"temperature\": 0.0,\n",
    "        \"top_p\": None\n",
    "    },\n",
    "    \"Shallow\": {\n",
    "        \"instruction\": (\n",
    "            \"Respond with a brief explanation or summary. You may use up to 2–3 paragraphs, \"\n",
    "            \"but avoid deep technical breakdowns or extended reasoning.\"\n",
    "        ),\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    },\n",
    "    \"Deep\": {\n",
    "        \"instruction\": (\n",
    "            \"Respond with a detailed and structured explanation. Walk through the reasoning steps, \"\n",
    "            \"make assumptions explicit, and provide a thorough breakdown.\"\n",
    "        ),\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "}\n",
    "\n",
    "conversation_history = []\n",
    "\n",
    "# ─── MAIN FUNCTION ───────────────────────────────────────────────\n",
    "def process_query_with_reasoning_and_answer(query: str, max_class_tokens: int = 10):\n",
    "    peft_model.set_adapter(CLASSIFIER_ADAPTER_NAME)\n",
    "\n",
    "    cls_prompt = (\n",
    "        \"<s>[INST] You are an expert that classifies query complexity.\\n\"\n",
    "        \"Respond with only one of: Simple, Shallow, or Deep. Do not explain and respond with EXACTLY ONE WORD.\\n\"\n",
    "        f\"Query: \\\"{query}\\\"\\n\"\n",
    "        \"Classification:[/INST]\"\n",
    "    )\n",
    "\n",
    "    inputs_cls = tokenizer(cls_prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs_cls = peft_model.generate(\n",
    "            **inputs_cls,\n",
    "            max_new_tokens=max_class_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    raw_cls = tokenizer.decode(outputs_cls[0][inputs_cls.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    level = clean_artifacts(raw_cls).strip().lower().rstrip(\".\").capitalize()\n",
    "    if level not in REASONING_CONFIG:\n",
    "        logging.warning(f\"⚠ Unexpected classification: '{level}', defaulting to 'Simple'\")\n",
    "        level = \"Simple\"\n",
    "\n",
    "    logging.info(f\"Classified reasoning level: {level}\")\n",
    "    config = REASONING_CONFIG[level]\n",
    "    instruction = config[\"instruction\"]\n",
    "\n",
    "    conversation_history.append({\"role\": \"system\", \"content\": instruction})\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "        messages=conversation_history,\n",
    "        max_tokens=config[\"max_new_tokens\"],\n",
    "        temperature=config[\"temperature\"],\n",
    "        top_p=config.get(\"top_p\")\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "    return level, answer\n",
    "\n",
    "# ─── CLI LOOP ────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_query = input(\"Enter your query (or 'exit' to quit): \").strip()\n",
    "        if user_query.lower() == \"exit\":\n",
    "            break\n",
    "        lvl, ans = process_query_with_reasoning_and_answer(user_query)\n",
    "        print(f\"\\nReasoning Level: {lvl}\\nAnswer: {ans}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de4197a-a580-416f-8c58-116c0a499761",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
